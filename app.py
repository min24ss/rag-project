import streamlit as st
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, OpenAI
from langchain.chains import RetrievalQA  # ê²½ë¡œ ìˆ˜ì •
from langchain.prompts import PromptTemplate
import os
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (ê²½ë¡œ ëª…ì‹œ)
load_dotenv(dotenv_path="/home/a09999/projects/rag_project/.env")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# Streamlit UI ì„¤ì •
st.title("AI ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ")
st.write("ì˜ˆìˆ ê°€ë“¤ì˜ AI ì €ì‘ê¶Œ ì¹¨í•´ ì†Œì†¡ ë“± ë‹¤ì–‘í•œ ì§ˆë¬¸ì„ ì…ë ¥í•´ ë³´ì„¸ìš”!")

# ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ
embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
vectorstore = FAISS.load_local("faiss_index", embeddings, allow_dangerous_deserialization=True)
st.success("âœ… ë²¡í„°ìŠ¤í† ì–´ê°€ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")

# RetrievalQA ì²´ì¸ ì„¤ì •
llm = OpenAI(openai_api_key=OPENAI_API_KEY, temperature=0)
custom_prompt = PromptTemplate(
    input_variables=["context", "question"],
    template="ë‹¤ìŒ ë¬¸ë§¥ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.\n\në¬¸ë§¥:\n{context}\n\nì§ˆë¬¸:\n{question}\n\në‹µë³€:"
)
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vectorstore.as_retriever(),
    chain_type="stuff",
    return_source_documents=True
)

# ì‚¬ìš©ì ì…ë ¥
query = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:", "")
if st.button("ì§ˆë¬¸í•˜ê¸°"):
    if query:
        result = qa_chain({"query": query})

        # Streamlitì„ ì‚¬ìš©í•´ ê²°ê³¼ ì¶œë ¥
        st.subheader("ğŸ¯ ë‹µë³€:")
        st.write(result["result"])

        # ê²€ìƒ‰ëœ ë¬¸ì„œ í™•ì¸
        st.subheader("ğŸ” ê²€ìƒ‰ëœ ë¬¸ì„œ:")
        for doc in result["source_documents"]:
            st.text(doc.page_content)
    else:
        st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”!")
